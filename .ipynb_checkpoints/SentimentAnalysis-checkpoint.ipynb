{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment analysis with scikit-learn</h1>\n",
    "<p>As part of my <a href=\"http://niconico.dk/files/Opinionmining_speciale.pdf\" target=\"_blank\">master thesis</a> in IT and Cognition from University of Copenhagen I worked on this pretty straight forward piece of code.</p>\n",
    "\n",
    "<p>From a collection of news articles on Danish bank Nykredit the relevant data is extracted, cleaned and used to train and test a few classifiers utilizing machine learning library scikit-learn.</p>\n",
    "\n",
    "<p>More than anything this is a test of the quality of the tagging of the data. And as the results show the quality ain't the best. The reasons for this is discussed in chapter 6.4.2 of the above mentioned thesis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# numpy and pandas for data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# BeautifulSoup for parsing XML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# nltk for various NLP tasks (Natural Language Toolkit)\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# difflib for measuring similarity of text\n",
    "from difflib import SequenceMatcher as textSimilarity\n",
    "\n",
    "# sklearn objects for feature extraction, classification and cross validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB as bnb\n",
    "\n",
    "print (\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NK_data loaded\n"
     ]
    }
   ],
   "source": [
    "def nykredit_xls_to_dataframe():\n",
    "    \"\"\"\n",
    "    Parses Nykredit_feed.xls and returns selected columns of the spreadsheet\n",
    "    as a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        NK_data = pd.read_pickle(\"NK_data.pkl\")\n",
    "        print (\"NK_data loaded\")\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        # loads the spreadsheet as a pandas.DataFrame\n",
    "        xls_file = pd.ExcelFile(\"Nykredit_feed.xls\")\n",
    "        nykredit_data = xls_file.parse('tmp62.tmp')\n",
    "        nykredit_text = xls_file.parse('ArticleText')\n",
    "        \n",
    "        # choose which columns to use from the spreadsheet:\n",
    "        selected_parameters = ['ArticleKey', 'ArticleDate', 'QualitativeScore',\n",
    "                               'Headline', 'Kilde', 'Raw Xml']\n",
    "        NK_data = nykredit_data.reindex(columns=selected_parameters)\n",
    "\n",
    "        # add a column containing the texts extracted from the xml:\n",
    "        NK_data['Text'] = NK_data.apply(extract_text_from_xml, axis=1)\n",
    "        \n",
    "        # change column name 'Kilde'=>'Source'\n",
    "        NK_data.columns = [['ArticleKey', 'ArticleDate', 'QualitativeScore',\n",
    "                            'Headline', 'Source', 'Raw Xml', 'Text']]\n",
    "        \n",
    "        NK_data.to_pickle(\"NK_data.pkl\")\n",
    "        print ('NK_data.pkl constructed and saved to file')\n",
    "        \n",
    "    return NK_data\n",
    "\n",
    "def extract_text_from_xml(row):\n",
    "    \"\"\"\n",
    "    Parses the raw XML string in row and returns the text/article contained in row.XML.\n",
    "    \"\"\"    \n",
    "    soup = BeautifulSoup(row['Raw Xml'], 'lxml')\n",
    "    p_blocks = soup.findAll('p')\n",
    "    \n",
    "    output = []\n",
    "    for p in p_blocks:\n",
    "        if p.string: # some p blocks contains a None element\n",
    "            output.append(p.string)\n",
    "    return \" \".join(output)\n",
    "\n",
    "NK_data = nykredit_xls_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now the NK_data frame looks as follows:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleKey</th>\n",
       "      <th>ArticleDate</th>\n",
       "      <th>QualitativeScore</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Raw Xml</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25f3cf8</td>\n",
       "      <td>2010-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>Fortsat lav rente på flexlån</td>\n",
       "      <td>Hadsund Folkeblad</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;NewsML ...</td>\n",
       "      <td>ALS: December måned ventes hvert år med spændi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e25bea65</td>\n",
       "      <td>2010-12-21</td>\n",
       "      <td>0</td>\n",
       "      <td>Fortsat en meget lav rente på et-årige flexlån</td>\n",
       "      <td>Aars Avis</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;NewsML ...</td>\n",
       "      <td>AARS: December ventes hvert år med spænding af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e252432b</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Førstegangskøbere kan med fordel købe bolig nu</td>\n",
       "      <td>Aabenraa Ugeavis</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;NewsML ...</td>\n",
       "      <td>BOLIG: » Mange førstegangskøbere kan med forde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e23bc88a</td>\n",
       "      <td>2010-10-05</td>\n",
       "      <td>1</td>\n",
       "      <td>Sparbank Nord anbefaler at lægge huslånene om</td>\n",
       "      <td>Annonce Bladet Salling-Fur-Skive</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;NewsML ...</td>\n",
       "      <td>I de seneste måneder har massevis af boligejer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e2520d69</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Nu kan der med fordel købes bolig</td>\n",
       "      <td>Lokal-Bladet Budstikken Vejen</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;NewsML ...</td>\n",
       "      <td>-Mange førstegangskøbere kan med fordel slå ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ArticleKey ArticleDate  QualitativeScore  \\\n",
       "0   e25f3cf8  2010-12-28                 0   \n",
       "1   e25bea65  2010-12-21                 0   \n",
       "2   e252432b  2010-12-01                 1   \n",
       "3   e23bc88a  2010-10-05                 1   \n",
       "4   e2520d69  2010-12-01                 1   \n",
       "\n",
       "                                         Headline  \\\n",
       "0                    Fortsat lav rente på flexlån   \n",
       "1  Fortsat en meget lav rente på et-årige flexlån   \n",
       "2  Førstegangskøbere kan med fordel købe bolig nu   \n",
       "3   Sparbank Nord anbefaler at lægge huslånene om   \n",
       "4               Nu kan der med fordel købes bolig   \n",
       "\n",
       "                             Source  \\\n",
       "0                 Hadsund Folkeblad   \n",
       "1                         Aars Avis   \n",
       "2                  Aabenraa Ugeavis   \n",
       "3  Annonce Bladet Salling-Fur-Skive   \n",
       "4     Lokal-Bladet Budstikken Vejen   \n",
       "\n",
       "                                             Raw Xml  \\\n",
       "0  <?xml version=\"1.0\" encoding=\"utf-8\"?><NewsML ...   \n",
       "1  <?xml version=\"1.0\" encoding=\"utf-8\"?><NewsML ...   \n",
       "2  <?xml version=\"1.0\" encoding=\"utf-8\"?><NewsML ...   \n",
       "3  <?xml version=\"1.0\" encoding=\"utf-8\"?><NewsML ...   \n",
       "4  <?xml version=\"1.0\" encoding=\"utf-8\"?><NewsML ...   \n",
       "\n",
       "                                                Text  \n",
       "0  ALS: December måned ventes hvert år med spændi...  \n",
       "1  AARS: December ventes hvert år med spænding af...  \n",
       "2  BOLIG: » Mange førstegangskøbere kan med forde...  \n",
       "3  I de seneste måneder har massevis af boligejer...  \n",
       "4  -Mange førstegangskøbere kan med fordel slå ti...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NK_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing duplicate and semi-duplicate texts</h3>\n",
    "<p>\n",
    "    Besides a few identical duplicates in the collection (which can be easily and quickly removed) there are also a bunch of semi-duplicates. An example of these could be an article from a news agency published in two different papers. The first paper prints the article in it's entirety whereas the second cuts the last paragraph. The texts aren't identical but they share enough similarities so that we can remove one of them in order to avoid redundant datapoints. And, of course, the degree of similarity determines whether a text is considered a semi-duplicate or not.\n",
    "</p>\n",
    "<p>\n",
    "    The similarity is measured using the <code><a href=\"https://docs.python.org/2/library/difflib.html\"> difflib.SequenceMatcher</a></code> class. It is a class for compairing and scoring similarity of sequences - texts in our case. The threshold is set to 0.95 and if two texts has a similarity score above this, one of them is removed while the other is kept in the data set.\n",
    "</p>\n",
    "<p>\n",
    "    <code>difflib.SequenceMatcher</code> (which has been loaded as <code>textSimilarity</code> in the import section) is a rather calculation heavy function. Hence\n",
    "    the nested while loop in order not to compare a text that has already been deemed duplicate to another unseen text.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates loaded\n"
     ]
    }
   ],
   "source": [
    "def duplicate_indexes(dataFrame):\n",
    "    \"\"\"\n",
    "    Returns the indexes of duplicate and semi duplicates texts in dataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        duplicates_list = pickle.load(open(\"duplicates\", \"rb\"))\n",
    "        print (\"duplicates loaded\")\n",
    "    except:\n",
    "        duplicates_list = []\n",
    "        i = 0\n",
    "        while i<dataFrame.shape[0]-1:\n",
    "            if not i in duplicates_list: # text[i] is a duplicate => no need to check\n",
    "                j = i+1\n",
    "                while j<dataFrame.shape[0]:\n",
    "                    if not j in duplicates_list: # text[j] is a duplicate => no need to check\n",
    "                        text_similarity = textSimilarity(None,\n",
    "                                                         dataFrame.ix[i].Text,\n",
    "                                                         dataFrame.ix[j].Text\n",
    "                                                        ).quick_ratio()\n",
    "                        if text_similarity > 0.95:\n",
    "                            duplicates_list.append(j)\n",
    "                            print((i, j))\n",
    "                            j += 1\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        pickle.dump(duplicates_list, open(\"duplicates\", \"wb\"))\n",
    "    return duplicates_list\n",
    "\n",
    "duplicates = duplicate_indexes(NK_data)\n",
    "NK_unique_data = pd.DataFrame.copy(NK_data.drop(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature vectors</h3>\n",
    "<p>\n",
    "    I'm using scikit-learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\"><code>CountVectorizer</code></a> class for counting the number of times the tokens (i.e. words) appear in the documents. The <code>vectorizer</code> is then used for building the feature vectors.\n",
    "</p>\n",
    "<p>\n",
    "    Finally <code>crossvalidate_algorithms</code> performs a k-fold stratified shuffled split validation of a list of classifiers on the set of feature vectors given in input.\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB : [ 0.80147059  0.84558824  0.83088235  0.79411765  0.83823529] \n",
      "Mean accuracy: 82.21 \n",
      "\n",
      "LinearSVC : [ 0.78676471  0.83823529  0.84558824  0.75        0.82352941] \n",
      "Mean accuracy: 80.88 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating feature vectors using sklearn\n",
    "\n",
    "def texts_to_feature_vectors(dataframe, number_of_features=5000):\n",
    "    \"\"\"\n",
    "    Based on the dataframe given in input, texts_to_feature_vectors builds and returns\n",
    "    an array of feature vectors representing the texts in the dataframe and an array of\n",
    "    the appertaining labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    NK_positive_negative = pd.DataFrame.copy(dataframe[dataframe.QualitativeScore!=-1])\n",
    "    target = np.array(NK_positive_negative.QualitativeScore) # list of corresponding labels\n",
    "    \n",
    "    texts = np.array(NK_positive_negative.Text)\n",
    "    \n",
    "    # Initializing the CountVectorizer object\n",
    "    # vectorizer counts the number of times the tokens appear in the document\n",
    "    vectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, \\\n",
    "                                 stop_words=stopwords.words(\"Danish\"), \\\n",
    "                                 max_features=number_of_features)\n",
    "\n",
    "    # Creating the feature vectors\n",
    "    feature_vectors = vectorizer.fit_transform(texts).toarray()\n",
    "    \n",
    "    return feature_vectors, target\n",
    "\n",
    "X, y = texts_to_feature_vectors(NK_unique_data)\n",
    "\n",
    "def crossvalidate_algorithms(algos, f_vectors, target):\n",
    "    \"\"\"\n",
    "    crossvalidate_algorithms performs a k-fold stratified shuffled split validation\n",
    "    of an array of classifiers on the set of feature vectors given in input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # StratifiedShuffleSplit is an iterator for generating stratified and\n",
    "    # shuffled splits for the cross validation\n",
    "    sss = StratifiedShuffleSplit(target, n_iter=5, test_size=0.1, random_state=0)\n",
    "    \n",
    "    scores = {}\n",
    "    for algo in algos:\n",
    "        s = cross_validation.cross_val_score(algo(), f_vectors, target, cv=sss)\n",
    "        scores[algo.__name__] = s\n",
    "        print (algo.__name__, \":\", s, \"\\nMean accuracy:\", round(100*s.mean(), 2), \"\\n\")\n",
    "    return scores\n",
    "\n",
    "algorithm_scores = crossvalidate_algorithms([bnb, svm.LinearSVC], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results</h3>\n",
    "<p>The accuracy scores are pretty low. The various reasons for this is discussed in chapter 6.4.2 in the aforementioned thesis, but one of the main reasons is wrongfully tagged texts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequencyDistribution.B() = 39920 , loCut = 0.1 , hiCut = 0.9\n",
      "15618\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# As of now these functions aren't used for anything. They're used for a finer\n",
    "# feature selection than what the vanilla equivalents of scikit-learn offers.\n",
    "#\n",
    "\n",
    "def createFreqDist(dataframe):\n",
    "    \"\"\"\n",
    "    Creates an nltk frequency distribution for the input dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    allTexts = \"\"\n",
    "    for index, row in dataframe.iterrows():\n",
    "        allTexts += row.Text + \" \"\n",
    "    frequencyDistribution = nltk.FreqDist(word_tokenize(allTexts))\n",
    "    return frequencyDistribution\n",
    "frequencyDistribution = createFreqDist(NK_unique_data)\n",
    "\n",
    "def trimFreqDist(freqDist, loCut, hiCut):\n",
    "    \"\"\"\n",
    "    trimFreqDist removes the least and most common tokens of the frequency distribution\n",
    "    according to the values defined by loCut and hiCut.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"frequencyDistribution.B() =\", frequencyDistribution.B(), \", loCut =\", loCut, \", hiCut =\", hiCut)\n",
    "    fd = frequencyDistribution.copy()\n",
    "    FreqDistList = fd.most_common(fd.B()) # Ordered list of (token, freq) from the most to least common tokens\n",
    "    for (token, freq) in FreqDistList: # Remove tokens that only appear once\n",
    "        if freq==1:\n",
    "            fd.pop(token)\n",
    "            \n",
    "    FreqDistList = fd.most_common(fd.B()) # Update list after removing token of frequency 1\n",
    "    loCutIndex = int(fd.B()*loCut) # The B() method returns the number of tokens in the freqDist\n",
    "    hiCutIndex = int(fd.B()*hiCut)\n",
    "    \n",
    "    for (token, freq) in FreqDistList[:loCutIndex]:\n",
    "        fd.pop(token)\n",
    "    for (token, freq) in FreqDistList[hiCutIndex:]:\n",
    "        fd.pop(token)\n",
    "    print(fd.B())\n",
    "    return fd\n",
    "\n",
    "trimmedFreqDist = trimFreqDist(frequencyDistribution, 0.1, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
