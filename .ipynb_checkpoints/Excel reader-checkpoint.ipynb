{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NK_data loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, xlrd, nltk, unicodedata, pickle, os, re, time\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher as textSimilarity # for measuring similarity of text\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup # for parsing XML\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def nykredit_xls_to_dataframe():\n",
    "    \"\"\"\n",
    "    Parses Nykredit_feed.xls and returns selected columns of the spreadsheet as a Pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        NK_data = pickle.load(open(\"NK_data\", \"rb\"))\n",
    "        print (\"NK_data loaded\")\n",
    "    except:        \n",
    "        # loads the spreadsheet as a pandas.DataFrame\n",
    "        xls_file = pd.ExcelFile(\"Nykredit_feed.xls\")\n",
    "#        print (xls_file.sheet_names)\n",
    "        nykredit_data = xls_file.parse('tmp62.tmp')\n",
    "        nykredit_text = xls_file.parse('ArticleText')\n",
    "        \n",
    "        # choose which columns to use from the spreadsheet:\n",
    "        selected_parameters = ['ArticleKey', 'ArticleDate', 'QualitativeScore', 'Headline', 'Kilde', 'Raw Xml']\n",
    "        NK_data = nykredit_data.reindex(columns=selected_parameters)\n",
    "\n",
    "        # add a column containing the texts extracted from the xml:\n",
    "        NK_data['Text'] = NK_data.apply(extract_text_from_xml, axis=1)\n",
    "        # change column name 'Kilde'=>'Source'\n",
    "        NK_data.columns = [['ArticleKey', 'ArticleDate', 'QualitativeScore', 'Headline', 'Source', 'Raw Xml', 'Text']]\n",
    "        pickle.dump(NK_data, open(\"NK_data\", \"wb\"))\n",
    "        \n",
    "    return NK_data\n",
    "\n",
    "def extract_text_from_xml(row):\n",
    "    \"\"\"\n",
    "    Parses the raw XML string in row and returns the text/article contained in row.XML\n",
    "    \"\"\"    \n",
    "    soup = BeautifulSoup(row['Raw Xml'], 'lxml')\n",
    "    p_blocks = soup.findAll('p')\n",
    "    \n",
    "    output = []\n",
    "    for p in p_blocks:\n",
    "        if p.string: # some p blocks contains a None element\n",
    "            output.append(p.string)\n",
    "    return \" \".join(output)\n",
    "\n",
    "def tokenize_text(row):\n",
    "    \"\"\"\n",
    "    Returns tokens in row.Text using nltk.word_tokenize\n",
    "    \"\"\"\n",
    "    return word_tokenize(row.Text)\n",
    "\n",
    "NK_data = nykredit_xls_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates loaded\n"
     ]
    }
   ],
   "source": [
    "def duplicate_indexes(dataFrame):\n",
    "    \"\"\"\n",
    "    Returns the indexes of duplicate and semi duplicates texts in dataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        duplicates_list = pickle.load(open(\"duplicates\", \"rb\"))\n",
    "        print (\"duplicates loaded\")\n",
    "    except:\n",
    "        duplicates_list = []\n",
    "        i = 0\n",
    "        while i<dataFrame.shape[0]-1:\n",
    "            if not i in duplicates_list: # text[i] is a duplicate => no need to check\n",
    "                j = i+1\n",
    "                while j<dataFrame.shape[0]:\n",
    "                    if not j in duplicates_list: # text[j] is a duplicate => no need to check\n",
    "                        text_similarity = textSimilarity(None, dataFrame.ix[i].Text, dataFrame.ix[j].Text).quick_ratio()\n",
    "                        if text_similarity > 0.95:\n",
    "                            duplicates_list.append(j)\n",
    "                            print((i, j))\n",
    "                            j += 1\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        pickle.dump(duplicates_list, open(\"duplicates\", \"wb\"))\n",
    "    return duplicates_list\n",
    "\n",
    "duplicates = duplicate_indexes(NK_data)\n",
    "NK_unique_data = pd.DataFrame.copy(NK_data.drop(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    714\n",
       " 0    642\n",
       "-1     37\n",
       "Name: QualitativeScore, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NK_unique_data.QualitativeScore.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neutral_index = np.arange(NK_unique_data.ix[(NK_unique_data.QualitativeScore==-1)].shape[0])\n",
    "NK_neutral = NK_unique_data.ix[(NK_unique_data.QualitativeScore==-1)]\n",
    "#NK_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39920"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createFreqDist(data):\n",
    "    allTexts = \"\"\n",
    "    for index, row in NK_unique_data.iterrows():\n",
    "        allTexts += row.Text + \" \"\n",
    "    frequencyDistribution = nltk.FreqDist(word_tokenize(allTexts))\n",
    "    return frequencyDistribution\n",
    "frequencyDistribution = createFreqDist(NK_unique_data)\n",
    "len(frequencyDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequencyDistribution.B() = 39920 , loCut = 0.1 , hiCut = 0.9\n",
      "15618\n"
     ]
    }
   ],
   "source": [
    "def trimFreqDist(freqDist, loCut, hiCut):\n",
    "    \"\"\"\n",
    "    trimFreqDist removes the least and most common tokens of the frequency distribution\n",
    "    according to the values defined by loCut and hiCut.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"frequencyDistribution.B() =\", frequencyDistribution.B(), \", loCut =\", loCut, \", hiCut =\", hiCut)\n",
    "    fd = frequencyDistribution.copy()\n",
    "    FreqDistList = fd.most_common(fd.B()) # Ordered list of (token, freq) from the most to least common tokens\n",
    "    for (token, freq) in FreqDistList:\n",
    "        if freq==1:\n",
    "            fd.pop(token)\n",
    "    FreqDistList = fd.most_common(fd.B()) # Update list after removing token of frequency 1\n",
    "    loCutIndex = int(fd.B()*loCut) # The B() method returns the number of tokens in the freqDist\n",
    "    hiCutIndex = int(fd.B()*hiCut)\n",
    "    \n",
    "    for (token, freq) in FreqDistList[:loCutIndex]:\n",
    "        fd.pop(token)\n",
    "    for (token, freq) in FreqDistList[hiCutIndex:]:\n",
    "        fd.pop(token)\n",
    "    print(fd.B())\n",
    "    return fd\n",
    "trimmedFreqDist = trimFreqDist(frequencyDistribution, 0.1, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
