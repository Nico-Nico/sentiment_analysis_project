{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# numpy and pandas for data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# BeautifulSoup for parsing XML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# nltk for various NLP tasks (Natural Language Toolkit)\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# difflib for measuring similarity of text\n",
    "from difflib import SequenceMatcher as textSimilarity\n",
    "\n",
    "# sklearn objects for feature extraction, classification and cross validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NK_data constructed and saved to file\n"
     ]
    }
   ],
   "source": [
    "def nykredit_xls_to_dataframe():\n",
    "    \"\"\"\n",
    "    Parses Nykredit_feed.xls and returns selected columns of the spreadsheet as a Pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        NK_data = pickle.load(open(\"NK_data\", \"rb\"))\n",
    "        print (\"NK_data loaded\")\n",
    "    except:\n",
    "        # loads the spreadsheet as a pandas.DataFrame\n",
    "        xls_file = pd.ExcelFile(\"Nykredit_feed.xls\")\n",
    "        nykredit_data = xls_file.parse('tmp62.tmp')\n",
    "        nykredit_text = xls_file.parse('ArticleText')\n",
    "        \n",
    "        # choose which columns to use from the spreadsheet:\n",
    "        selected_parameters = ['ArticleKey', 'ArticleDate', 'QualitativeScore',\n",
    "                               'Headline', 'Kilde', 'Raw Xml']\n",
    "        NK_data = nykredit_data.reindex(columns=selected_parameters)\n",
    "\n",
    "        # add a column containing the texts extracted from the xml:\n",
    "        NK_data['Text'] = NK_data.apply(extract_text_from_xml, axis=1)\n",
    "        \n",
    "        # change column name 'Kilde'=>'Source'\n",
    "        NK_data.columns = [['ArticleKey', 'ArticleDate', 'QualitativeScore',\n",
    "                            'Headline', 'Source', 'Raw Xml', 'Text']]\n",
    "        pickle.dump(NK_data, open(\"NK_data\", \"wb\"))\n",
    "        print ('NK_data constructed and saved to file')\n",
    "        \n",
    "    return NK_data\n",
    "\n",
    "def extract_text_from_xml(row):\n",
    "    \"\"\"\n",
    "    Parses the raw XML string in row and returns the text/article contained in row.XML\n",
    "    \"\"\"    \n",
    "    soup = BeautifulSoup(row['Raw Xml'], 'lxml')\n",
    "    p_blocks = soup.findAll('p')\n",
    "    \n",
    "    output = []\n",
    "    for p in p_blocks:\n",
    "        if p.string: # some p blocks contains a None element\n",
    "            output.append(p.string)\n",
    "    return \" \".join(output)\n",
    "\n",
    "NK_data = nykredit_xls_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates loaded\n"
     ]
    }
   ],
   "source": [
    "def duplicate_indexes(dataFrame):\n",
    "    \"\"\"\n",
    "    Returns the indexes of duplicate and semi duplicates texts in dataFrame.\n",
    "    \n",
    "    difflib.SequenceMatcher (textSimilarity) is a rather calculation heavy function. Hence\n",
    "    the nested while loop in order not to compare a text that has been deemed a duplicate\n",
    "    to another unseen text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        duplicates_list = pickle.load(open(\"duplicates\", \"rb\"))\n",
    "        print (\"duplicates loaded\")\n",
    "    except:\n",
    "        duplicates_list = []\n",
    "        i = 0\n",
    "        while i<dataFrame.shape[0]-1:\n",
    "            if not i in duplicates_list: # text[i] is a duplicate => no need to check\n",
    "                j = i+1\n",
    "                while j<dataFrame.shape[0]:\n",
    "                    if not j in duplicates_list: # text[j] is a duplicate => no need to check\n",
    "                        text_similarity = textSimilarity(None, dataFrame.ix[i].Text, dataFrame.ix[j].Text).quick_ratio()\n",
    "                        if text_similarity > 0.95:\n",
    "                            duplicates_list.append(j)\n",
    "                            print((i, j))\n",
    "                            j += 1\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        pickle.dump(duplicates_list, open(\"duplicates\", \"wb\"))\n",
    "    return duplicates_list\n",
    "\n",
    "duplicates = duplicate_indexes(NK_data)\n",
    "NK_unique_data = pd.DataFrame.copy(NK_data.drop(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target length: (1356,)\n",
      "fv length: (1356, 5000) \n",
      "\n",
      "BernoulliNB : [ 0.80147059  0.84558824  0.83088235  0.79411765  0.83823529] \n",
      "Mean accuracy: 82.21 \n",
      "\n",
      "LinearSVC : [ 0.78676471  0.83823529  0.84558824  0.75        0.82352941] \n",
      "Mean accuracy: 80.88 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating feature vectors using sklearn\n",
    "\n",
    "def texts_to_feature_vectors(dataframe):\n",
    "    \"\"\"\n",
    "    Based on the dataframe given in input, texts_to_feature_vectors builds and returns\n",
    "    an array of feature vectors representing the texts in the dataframe and an array of\n",
    "    the appertaining labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    NK_positive_negative = pd.DataFrame.copy(dataframe[dataframe.QualitativeScore!=-1])\n",
    "    target = np.array(NK_positive_negative.QualitativeScore) # list of corresponding labels\n",
    "    print(\"target length:\", target.shape)\n",
    "    \n",
    "    texts = np.array(NK_positive_negative.Text)\n",
    "    \n",
    "    # Initializing the CountVectorizer object\n",
    "    # vectorizer counts the number of times the tokens appear in the document\n",
    "    vectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, \\\n",
    "                                 stop_words=stopwords.words(\"Danish\"), max_features=5000)\n",
    "\n",
    "    # Creating the feature vectors\n",
    "    feature_vectors = vectorizer.fit_transform(texts).toarray()\n",
    "    print (\"fv length:\", feature_vectors.shape, \"\\n\")\n",
    "    \n",
    "    return feature_vectors, target\n",
    "\n",
    "X, y = texts_to_feature_vectors(NK_unique_data)\n",
    "\n",
    "def crossvalidate_algorithms(algos, f_vectors, target):\n",
    "    \"\"\"\n",
    "    crossvalidate_algorithms performs a k-fold stratified shuffled split validation\n",
    "    of an array of classifiers on the set of feature vectors given in input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # StratifiedShuffleSplit is an iterator for generating stratified and\n",
    "    # shuffled splits for the cross validation\n",
    "    sss = StratifiedShuffleSplit(target, n_iter=5, test_size=0.1, random_state=0)\n",
    "    \n",
    "    scores = {}\n",
    "    for algo in algos:\n",
    "        s = cross_validation.cross_val_score(algo(), f_vectors, target, cv=sss)\n",
    "        scores[algo.__name__] = s\n",
    "        print (algo.__name__, \":\", s, \"\\nMean accuracy:\", round(100*s.mean(), 2), \"\\n\")\n",
    "    return scores\n",
    "\n",
    "algorithm_scores = crossvalidate_algorithms([bnb, svm.LinearSVC], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequencyDistribution.B() = 39920 , loCut = 0.1 , hiCut = 0.9\n",
      "15618\n"
     ]
    }
   ],
   "source": [
    "def createFreqDist(dataframe):\n",
    "    \"\"\"\n",
    "    Creates an nltk frequency distribution for the input dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    allTexts = \"\"\n",
    "    for index, row in dataframe.iterrows():\n",
    "        allTexts += row.Text + \" \"\n",
    "    frequencyDistribution = nltk.FreqDist(word_tokenize(allTexts))\n",
    "    return frequencyDistribution\n",
    "frequencyDistribution = createFreqDist(NK_unique_data)\n",
    "\n",
    "def trimFreqDist(freqDist, loCut, hiCut):\n",
    "    \"\"\"\n",
    "    trimFreqDist removes the least and most common tokens of the frequency distribution\n",
    "    according to the values defined by loCut and hiCut.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"frequencyDistribution.B() =\", frequencyDistribution.B(), \", loCut =\", loCut, \", hiCut =\", hiCut)\n",
    "    fd = frequencyDistribution.copy()\n",
    "    FreqDistList = fd.most_common(fd.B()) # Ordered list of (token, freq) from the most to least common tokens\n",
    "    for (token, freq) in FreqDistList: # Remove tokens that only appear once\n",
    "        if freq==1:\n",
    "            fd.pop(token)\n",
    "            \n",
    "    FreqDistList = fd.most_common(fd.B()) # Update list after removing token of frequency 1\n",
    "    loCutIndex = int(fd.B()*loCut) # The B() method returns the number of tokens in the freqDist\n",
    "    hiCutIndex = int(fd.B()*hiCut)\n",
    "    \n",
    "    for (token, freq) in FreqDistList[:loCutIndex]:\n",
    "        fd.pop(token)\n",
    "    for (token, freq) in FreqDistList[hiCutIndex:]:\n",
    "        fd.pop(token)\n",
    "    print(fd.B())\n",
    "    return fd\n",
    "\n",
    "trimmedFreqDist = trimFreqDist(frequencyDistribution, 0.1, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
