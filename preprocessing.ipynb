{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementer i docs_raw_XML:  2147\n",
      "Antal artikler: 2146\n",
      "Antal artikler (efter dubletter fjernet): 1817\n",
      "Antal neutrale artikler: 55\n",
      "Artikler fordelt klassevist, efter de neutrale er fjernet:\n",
      "\tpos: 961 \n",
      "\tneg: 801 \n",
      "\tneutral: 0\n",
      "Samlet processeringstid: 1.3798458576202393\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp1252 -*-\n",
    "import string, xlrd, nltk, unicodedata, pickle, os, re, time\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# indlæser Excel-arket med xlrd\n",
    "nykredit = xlrd.open_workbook('Nykredit_feed.xls')\n",
    "nykredit_data = nykredit.sheet_by_index(0)\n",
    "nykredit_tekst = nykredit.sheet_by_index(1)\n",
    "\n",
    "docs_raw_XML = {} # dict-entries af formen docs_raw_XML[id] = xml\n",
    "docs_data_temp = []\n",
    "for i in range(0,2333):\n",
    "    docs_raw_XML[nykredit_tekst.row_values(i)[0]] = nykredit_tekst.row_values(i)[1]\n",
    "    try:\n",
    "        docs_data_temp.append(nykredit_data.row_values(i))\n",
    "    except:\n",
    "        pass\n",
    "del docs_raw_XML[''] # xls-filen indeholder dokumenter, der ikke har noget id\n",
    "print (\"Elementer i docs_raw_XML: \", len(docs_raw_XML))\n",
    "\n",
    "docs_data = []\n",
    "# docs_data_temp indeholder dubletter. Disse bliver fjernet i docs_data\n",
    "for key in docs_raw_XML.keys():\n",
    "    for doc in docs_data_temp:\n",
    "        if doc[2] == key:\n",
    "            docs_data.append(doc)\n",
    "            break\n",
    "\n",
    "class artikel:\n",
    "    def __init__(self, tekst = \"\", overskrift = \"\", kategori = \"\", kilde = \"\", id_kode = \"\",\n",
    "                 klasse = \"\"):\n",
    "        self.tekst = tekst\n",
    "        self.overskrift = overskrift\n",
    "        self.kategori = kategori\n",
    "        self.kilde = kilde\n",
    "        self.id_kode = id_kode\n",
    "        self.klasse = klasse\n",
    "\n",
    "artikler = [] # liste af artikel-objekter\n",
    "error_list = [] # liste, der indeholder dokumenter, der ikke kan parses med regex\n",
    "regex = re.compile(\"<block[^>]*>(.*)</block>\", re.UNICODE|re.DOTALL)\n",
    "for doc in docs_data:\n",
    "    xml = docs_raw_XML[doc[2]]\n",
    "    clean_xml = xml.replace('\\\\', '/')\n",
    "    try:\n",
    "        mobj = regex.search(clean_xml)\n",
    "        content = mobj.groups()[0]\n",
    "        content = re.sub('<[^>]*>', '', content)\n",
    "        artikler.append(artikel(tekst=content, id_kode=doc[2], kategori=doc[1],\n",
    "                                kilde=doc[8], overskrift=doc[11], klasse=doc[15]))\n",
    "    except:\n",
    "        error_list.append((clean_xml, doc[0]))\n",
    "print (\"Antal artikler:\", len(artikler))\n",
    "\n",
    "# fjerner dubletter (hvad content angår)\n",
    "artikler_content = []\n",
    "artikler_filtered = []\n",
    "artikler_duplicates = {} # registrerer dubletter og deres id og klasse\n",
    "artikler_neutral = []\n",
    "for a in artikler:\n",
    "    if not a.tekst in artikler_content:\n",
    "        artikler_content.append(a.tekst)\n",
    "        if a.klasse == -1:\n",
    "            artikler_neutral.append(a)\n",
    "        else:\n",
    "            artikler_filtered.append(a)\n",
    "    else:\n",
    "        try:\n",
    "            artikler_duplicates[a.tekst][\"id\"].append(a.id_kode)\n",
    "            artikler_duplicates[a.tekst][\"polarities\"].append(a.klasse)\n",
    "        except:\n",
    "            artikler_duplicates[a.tekst] = {\"id\": a.id_kode,\n",
    "                                                 \"polarities\": a.klasse}\n",
    "                                                 \n",
    "\"\"\"\n",
    "\n",
    "# counters, der registrerer hvor mange dubletter af hver klasse\n",
    "neg_count = 0\n",
    "pos_count = 0\n",
    "neu_count = 0\n",
    "for a in artikler_duplicates:\n",
    "    if artikler_duplicates[a][\"polarities\"][0] == 0:\n",
    "        neg_count += len(artikler_duplicates[a][\"polarities\"])\n",
    "    if artikler_duplicates[a][\"polarities\"][0] == 1:\n",
    "        pos_count += len(artikler_duplicates[a][\"polarities\"])\n",
    "    if artikler_duplicates[a][\"polarities\"][0] == -1:\n",
    "        neu_count += len(artikler_duplicates[a][\"polarities\"])\n",
    "print (\"Dubletter:\\n\\tpos:\", pos_count, \"\\n\\tneg:\", neg_count, \"\\n\\tneu:\", neu_count)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "artikler = artikler_filtered\n",
    "\n",
    "print (\"Antal artikler (efter dubletter fjernet):\", len(artikler)+len(artikler_neutral))\n",
    "print (\"Antal neutrale artikler:\", len(artikler_neutral))\n",
    "\n",
    "f = open(\"artikler\", \"wb\") # gemmer artikler i en fil\n",
    "pickle.dump(artikler, f)\n",
    "f.close()\n",
    "\n",
    "# oversigt over fordelingen af artikler klassevist\n",
    "pos_docs = []\n",
    "neg_docs = []\n",
    "neutral_docs = []\n",
    "for doc in artikler:\n",
    "    if doc.klasse == -1:\n",
    "        neutral_docs.append(doc)\n",
    "    if doc.klasse == 1:\n",
    "        pos_docs.append(doc)\n",
    "    if doc.klasse == 0:\n",
    "        neg_docs.append(doc)\n",
    "print (\"Artikler fordelt klassevist, efter de neutrale er fjernet:\")\n",
    "print (\"\\tpos:\", len(pos_docs), \"\\n\\tneg:\", len(neg_docs), \"\\n\\tneutral:\", len(neutral_docs))\n",
    "\n",
    "kategorier = {}\n",
    "# laver en oversigt over hvor mange dokumenter af de forskellige kategorier:\n",
    "for doc in artikler:\n",
    "    if doc.kategori in kategorier.keys():\n",
    "        kategorier[(doc.kategori)] += 1\n",
    "    else:\n",
    "       kategorier[(doc.kategori)] = 1 \n",
    "\n",
    "f = open(\"kategorier\", \"wb\") # gemmer kategorier i en fil\n",
    "pickle.dump(kategorier, f)\n",
    "f.close()\n",
    "\n",
    "#f = open('ia_artikler', 'r')\n",
    "#ia_artikler = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "# frekvensdistribution af termerne i dokumentsamlingen\n",
    "leksikon = {}\n",
    "for doc in artikler:\n",
    "    content_clean = re.sub('[\"-,.:;\\'!?]', '', doc.tekst)\n",
    "    for term in content_clean.split():\n",
    "        term = term.lower()\n",
    "        if term in leksikon:\n",
    "            leksikon[term] += 1\n",
    "        else:\n",
    "            leksikon[term] = 1\n",
    "\n",
    "f = open(\"leksikon\", \"wb\") # gemmer leksikon i en fil\n",
    "pickle.dump(leksikon, f)\n",
    "f.close()\n",
    "\n",
    "termer = list(leksikon.keys())\n",
    "f = open(\"termer\", \"wb\") # gemmer termer i en fil\n",
    "pickle.dump(termer, f)\n",
    "f.close()\n",
    "\n",
    "end = time.time()\n",
    "print (\"Samlet processeringstid:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xlrd.sheet.Sheet at 0x10fa08eb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nykredit_tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
